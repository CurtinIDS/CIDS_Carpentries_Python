{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIDS Carpentries Workshop - Episode 10 - Defensive Programming\n",
    "This lesson is adapted from the Software Carpentries [Programming with Python](https://swcarpentry.github.io/python-novice-inflammation/index.html) lesson.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## â“ Questions and Objectives\n",
    "What should you be able to answer by the end of this episode?\n",
    "\n",
    "### Questions\n",
    "- How can I make my programs more reliable?\n",
    "\n",
    "\n",
    "### Objectives\n",
    "- Explain what an assertion is.\n",
    "- Add assertions that check the programâ€™s state is correct.\n",
    "- Correctly add precondition and postcondition assertions to functions.\n",
    "- Explain what test-driven development is, and use it when creating new functions.\n",
    "- Explain why variables should be initialized using actual data values rather than arbitrary constants.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our previous lessons have introduced the basic tools of programming: variables and lists, file I/O, loops, conditionals, and functions. \n",
    "\n",
    "What they *havenâ€™t* done is show us how to tell whether a program is getting the right answer, and how to tell if itâ€™s *still* getting the right answer as we make changes to it.\n",
    "\n",
    "To achieve that, we need to:\n",
    "\n",
    "1. Write programs that check their own operation.\n",
    "2. Write and run tests for widely-used functions.\n",
    "3. Make sure we know what â€œcorrectâ€ actually means.\n",
    "\n",
    "The good news is, doing these things will speed up our programming, not slow it down. As in real carpentry â€” the kind done with lumber â€” the time saved by measuring carefully before cutting a piece of wood is much greater than the time that measuring takes.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assertions\n",
    "\n",
    "The first step toward getting the right answers from our programs is to assume that mistakes **will** happen and to guard against them. This is called defensive programming, and the most common way to do it is to add assertions to our code so that it checks itself as it runs. \n",
    "\n",
    "An assertion is simply a statement that something must be true at a certain point in a program. When Python sees one, it evaluates the assertionâ€™s condition. If itâ€™s true, Python does nothing, but if itâ€™s false, Python halts the program immediately and prints the error message if one is provided. \n",
    "\n",
    "For example, this piece of code halts as soon as the loop encounters a value that isnâ€™t positive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers = [1.5, 2.3, 0.7, -0.001, 4.4]\n",
    "total = 0.0\n",
    "for num in numbers:\n",
    "    assert num > 0.0, 'Data should only contain positive values'\n",
    "    total += num\n",
    "print('total is:', total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Programs like the Firefox browser are full of assertions: 10-20% of the code they contain are there to check that the other 80â€“90% are working correctly. Broadly speaking, assertions fall into three categories:\n",
    "\n",
    "1. A precondition is something that must be true at the start of a function in order for it to work correctly.\n",
    "\n",
    "2. A postcondition is something that the function guarantees is true when it finishes.\n",
    "\n",
    "3. An invariant is something that is always true at a particular point inside a piece of code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, suppose we are representing rectangles using a tuple of four coordinates `(x0, y0, x1, y1)`, representing the lower left and upper right corners of the rectangle. In order to do some calculations, we need to normalise the rectangle so that the lower left corner is at the origin and the longest side is 1.0 units long. \n",
    "\n",
    "This function does that, but checks that its input is correctly formatted and that its result makes sense:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise_rectangle(rect):\n",
    "    \"\"\"Normalizes a rectangle so that it is at the origin and 1.0 units long on its longest axis.\n",
    "    Input should be of the format (x0, y0, x1, y1).\n",
    "    (x0, y0) and (x1, y1) define the lower left and upper right corners\n",
    "    of the rectangle, respectively.\"\"\"\n",
    "    assert len(rect) == 4, 'Rectangles must contain 4 coordinates'\n",
    "    x0, y0, x1, y1 = rect\n",
    "    assert x0 < x1, 'Invalid X coordinates'\n",
    "    assert y0 < y1, 'Invalid Y coordinates'\n",
    "\n",
    "    dx = x1 - x0\n",
    "    dy = y1 - y0\n",
    "    if dx > dy:\n",
    "        scaled = dx / dy\n",
    "        upper_x, upper_y = 1.0, scaled\n",
    "    else:\n",
    "        scaled = dx / dy\n",
    "        upper_x, upper_y = scaled, 1.0\n",
    "\n",
    "    assert 0 < upper_x <= 1.0, 'Calculated upper X coordinate invalid'\n",
    "    assert 0 < upper_y <= 1.0, 'Calculated upper Y coordinate invalid'\n",
    "\n",
    "    return (0, 0, upper_x, upper_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preconditions on lines 6, 8, and 9 catch invalid inputs:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(normalise_rectangle( (0.0, 1.0, 2.0) )) # missing the fourth coordinate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(normalise_rectangle( (4.0, 2.0, 1.0, 5.0) )) # X axis inverted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The post-conditions on `lines 20 and 21` help us catch bugs by telling us when our calculations might have been incorrect. \n",
    "\n",
    "For example, if we normalise a rectangle that is taller than it is wide everything seems OK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(normalise_rectangle( (0.0, 0.0, 1.0, 5.0) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "but if we normalise one thatâ€™s wider than it is tall, the assertion is triggered:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(normalise_rectangle( (0.0, 0.0, 5.0, 1.0) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-reading our function, we realize that line 14 should divide `dy` by `dx` rather than `dx` by `dy`. In a Jupyter notebook, you can display line numbers by typing `Ctrl+M` followed by `L`. If we had left out the assertion at the end of the function, we would have created and returned something that had the right shape as a valid answer, but wasnâ€™t. Detecting and debugging that would almost certainly have taken more time in the long run than writing the assertion.\n",
    "\n",
    "But assertions arenâ€™t just about catching errors: they also help people understand programs. Each assertion gives the person reading the program a chance to check (consciously or otherwise) that their understanding matches what the code is doing.\n",
    "\n",
    "Most good programmers follow two rules when adding assertions to their code. The first is, *fail early, fail often*. The greater the distance between when and where an error occurs and when itâ€™s noticed, the harder the error will be to debug, so good code catches mistakes as early as possible.\n",
    "\n",
    "The second rule is, *turn bugs into assertions or tests*. Whenever you fix a bug, write an assertion that catches the mistake should you make it again. If you made a mistake in a piece of code, the odds are good that you have made other mistakes nearby, or will make the same mistake (or a related one) the next time you change it. \n",
    "\n",
    "Writing assertions to check that you havenâ€™t regressed (i.e., havenâ€™t re-introduced an old problem) can save a lot of time in the long run, and helps to warn people who are reading the code (including your future self) that this bit is tricky.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test-Driven Development\n",
    "\n",
    "An assertion checks that something is true at a particular point in the program. The next step is to check the overall behavior of a piece of code, i.e., to make sure that it produces the right output when itâ€™s given a particular input. \n",
    "\n",
    "For example, suppose we need to find where two or more time series overlap. The range of each time series is represented as a pair of numbers, which are the time the interval started and ended. The output is the largest range that they all include:\n",
    "\n",
    "![overlapping-ranges](../assets/python-overlapping-ranges.svg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most novice programmers would solve this problem like this:\n",
    "\n",
    "1. Write a function `range_overlap`.\n",
    "2. Call it interactively on two or three different inputs.\n",
    "3. If it produces the wrong answer, fix the function and re-run that test.\n",
    "\n",
    "This clearly works â€” after all, thousands of scientists are doing it right now â€” but thereâ€™s a better way:\n",
    "\n",
    "1. Write a short function for each test.\n",
    "2. Write a `range_overlap` function that should pass those tests.\n",
    "3. If `range_overlap` produces any wrong answers, fix it and re-run the test functions.\n",
    "\n",
    "Writing the tests before writing the function they exercise is called test-driven development (TDD). Its advocates believe it produces better code faster because:\n",
    "\n",
    "1. If people write tests after writing the thing to be tested, they are subject to confirmation bias, i.e., they subconsciously write tests to show that their code is correct, rather than to find errors.\n",
    "2. Writing tests helps programmers figure out what the function is actually supposed to do.\n",
    "\n",
    "We start by defining an empty function `range_overlap`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def range_overlap(ranges):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are three test statements for `range_overlap`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert range_overlap([ (0.0, 1.0) ]) == (0.0, 1.0)\n",
    "assert range_overlap([ (2.0, 3.0), (2.0, 4.0) ]) == (2.0, 3.0)\n",
    "assert range_overlap([ (0.0, 1.0), (0.0, 2.0), (-1.0, 1.0) ]) == (0.0, 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error is actually reassuring: we havenâ€™t implemented any logic into `range_overlap` yet, so if the tests passed, it would indicate that weâ€™ve written an entirely ineffective test.\n",
    "\n",
    "And as a bonus of writing these tests, weâ€™ve implicitly defined what our input and output look like: we expect a list of pairs as input, and produce a single pair as output.\n",
    "\n",
    "Something important is missing, though. We donâ€™t have any tests for the case where the ranges donâ€™t overlap at all:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert range_overlap([ (0.0, 1.0), (5.0, 6.0) ]) == ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What should `range_overlap` do in this case: fail with an error message, produce a special value like (0.0, 0.0) to signal that thereâ€™s no overlap, or something else? \n",
    "\n",
    "Any actual implementation of the function will do one of these things; writing the tests first helps us figure out which is best before weâ€™re emotionally invested in whatever we happened to write before we realised there was an issue.\n",
    "\n",
    "And what about this case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert range_overlap([ (0.0, 1.0), (1.0, 2.0) ]) == ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do two segments that touch at their endpoints overlap or not? Mathematicians usually say â€œyesâ€, but engineers usually say â€œnoâ€. The best answer is â€œwhatever is most useful in the rest of our programâ€, but again, any actual implementation of `range_overlap` is going to do something, and whatever it is ought to be consistent with what it does when thereâ€™s no overlap at all.\n",
    "\n",
    "Since weâ€™re planning to use the range this function returns as the X axis in a time series chart, we decide that:\n",
    "\n",
    "1. every overlap has to have non-zero width, and\n",
    "2. we will return the special value `None` when thereâ€™s no overlap.\n",
    "\n",
    "`None` is built into Python, and means â€œnothing hereâ€. (Other languages often call the equivalent value `null` or `nil`). With that decision made, we can finish writing our last two tests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert range_overlap([ (0.0, 1.0), (5.0, 6.0) ]) == None\n",
    "assert range_overlap([ (0.0, 1.0), (1.0, 2.0) ]) == None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we get an error because we havenâ€™t written our function, but weâ€™re now ready to do so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def range_overlap(ranges):\n",
    "    \"\"\"Return common overlap among a set of [left, right] ranges.\"\"\"\n",
    "    max_left = 0.0\n",
    "    min_right = 1.0\n",
    "    for (left, right) in ranges:\n",
    "        max_left = max(max_left, left)\n",
    "        min_right = min(min_right, right)\n",
    "    return (max_left, min_right)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a moment to think about why we calculate the left endpoint of the overlap as the maximum of the input left endpoints, and the overlap right endpoint as the minimum of the input right endpoints. \n",
    "\n",
    "Weâ€™d now like to re-run our tests, but theyâ€™re scattered across three different cells. To make running them easier, letâ€™s put them all in a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_range_overlap():\n",
    "    assert range_overlap([ (0.0, 1.0), (5.0, 6.0) ]) == None\n",
    "    assert range_overlap([ (0.0, 1.0), (1.0, 2.0) ]) == None\n",
    "    assert range_overlap([ (0.0, 1.0) ]) == (0.0, 1.0)\n",
    "    assert range_overlap([ (2.0, 3.0), (2.0, 4.0) ]) == (2.0, 3.0)\n",
    "    assert range_overlap([ (0.0, 1.0), (0.0, 2.0), (-1.0, 1.0) ]) == (0.0, 1.0)\n",
    "    assert range_overlap([]) == None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now test range_overlap with a single function call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_range_overlap()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first test that was supposed to produce `None` fails, so we know something is wrong with our function. \n",
    "\n",
    "We *donâ€™t* know whether the other tests passed or failed because Python halted the program as soon as it spotted the first error. Still, some information is better than none, and if we trace the behavior of the function with that input, we realize that weâ€™re initialising `max_left` and `min_right` to 0.0 and 1.0 respectively, regardless of the input values. \n",
    "\n",
    "This violates another important rule of programming: *always initialize from data*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ† Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### âœï¸ Exercise 1 : Pre- and Post- Conditions\n",
    "\n",
    "Suppose you are writing a function called `average` that calculates the average of the numbers in a NumPy array. \n",
    "\n",
    "What pre-conditions and post-conditions would you write for it? Compare your answer to your neighborâ€™s: can you think of a function that will pass your tests but not his/hers or vice versa?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### âœï¸ Exercise 2 : Testing Assertions\n",
    "\n",
    "Given a sequence of a number of cars, the function `get_total_cars` returns the total number of cars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_total_cars(values):\n",
    "    assert len(values) > 0\n",
    "    for element in values:\n",
    "        assert int(element)\n",
    "    values = [int(element) for element in values]\n",
    "    total = sum(values)\n",
    "    assert total > 0\n",
    "    return total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain in words what the assertions in this function check, and for each one, give an example of input that will make that assertion fail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_total_cars([1, 2, 3, 4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_total_cars([\"a\", \"b\", \"c\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ”‘ Key Points\n",
    "- Program defensively, i.e., assume that errors are going to arise, and write code to detect them when they do.\n",
    "- Put assertions in programs to check their state as they run, and to help readers understand how those programs are supposed to work.\n",
    "- Use preconditions to check that the inputs to a function are safe to use.\n",
    "- Use postconditions to check that the output from a function is safe to use.\n",
    "- Write tests before writing code in order to help determine exactly what that code is supposed to do.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
